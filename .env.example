# Lab-Verse Agent - Configuration with Multi-Provider Support
# Copy this to .env.production and fill in the provider you want to use

# ===== PROVIDER SELECTION =====
# Choose which LLM provider to use:
# - "huggingface" (local models, requires download)
# - "z_ai" (Z.AI API - use GLM-4.7)
# - "qwen" (Qwen/Alibaba Dashscope)
LLM_PROVIDER=z_ai

# ===== BITBUCKET CONFIGURATION =====
PIPELINE_PLATFORM=bitbucket
BITBUCKET_WORKSPACE=lab-verse-monitaring
BITBUCKET_USERNAME=your-email@atlassian.com
BITBUCKET_APP_PASSWORD=your-64-character-app-password

# ===== Z.AI CONFIGURATION (if LLM_PROVIDER=z_ai) =====
# Use GLM-4.7 on Z.AI by setting model to "glm-4.7".
Z_AI_API_KEY=your-z-ai-api-key
Z_AI_MODEL_DIAGNOSTIC=glm-4.7
Z_AI_MODEL_PLANNER=glm-4.7
Z_AI_MODEL_EXECUTOR=glm-4.7
Z_AI_MODEL_VALIDATOR=glm-4.7

# ===== QWEN/DASHSCOPE CONFIGURATION (if LLM_PROVIDER=qwen) =====
QWEN_API_KEY=your-qwen-dashscope-api-key
QWEN_MODEL_DIAGNOSTIC=qwen-max
QWEN_MODEL_PLANNER=qwen-max
QWEN_MODEL_EXECUTOR=qwen-turbo
QWEN_MODEL_VALIDATOR=qwen-max

# ===== HUGGING FACE CONFIGURATION (if LLM_PROVIDER=huggingface) =====
# Only needed if using local Hugging Face models.
# Recommended default is DeepSeek-R1 distilled 7B (strong reasoning, manageable size).
HF_TOKEN=your-huggingface-api-key
HF_DEVICE=cuda
HF_LOAD_IN_8BIT=true
HF_LOAD_IN_4BIT=false
HF_CACHE_DIR=./models
HF_MODEL_DIAGNOSTIC=deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
HF_MODEL_PLANNER=deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
HF_MODEL_EXECUTOR=deepseek-ai/DeepSeek-R1-Distill-Qwen-7B
HF_MODEL_VALIDATOR=deepseek-ai/DeepSeek-R1-Distill-Qwen-7B

# ===== AGENT CONFIGURATION =====
ENVIRONMENT=production
LOG_LEVEL=INFO
MAX_RETRIES=3
APPROVAL_TIMEOUT_SECONDS=3600
PIPELINE_WAIT_TIMEOUT_SECONDS=1800

# ===== SECURITY =====
ENABLE_HUMAN_APPROVAL=true
ENABLE_AUDIT_LOGGING=true
ENABLE_RATE_LIMITING=true

# ===== INFRASTRUCTURE =====
KUBERNETES_NAMESPACE=lab-verse-monitoring
ENABLE_METRICS=true
METRICS_PORT=8001
