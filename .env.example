# Lab-Verse Agent - Configuration with Multi-Provider Support
# Copy this to .env.production and fill in the provider you want to use

# ===== PROVIDER SELECTION =====
# Choose which LLM provider to use:
# - "huggingface" (local models, requires download)
# - "z_ai" (Z.AI API - has Claude models)
# - "qwen" (Qwen/Alibaba Dashscope)
LLM_PROVIDER=z_ai

# ===== BITBUCKET CONFIGURATION =====
PIPELINE_PLATFORM=bitbucket
BITBUCKET_WORKSPACE=lab-verse-monitaring
BITBUCKET_USERNAME=your-email@atlassian.com
BITBUCKET_APP_PASSWORD=your-64-character-app-password

# ===== Z.AI CONFIGURATION (if LLM_PROVIDER=z_ai) =====
# ‚≠ê RECOMMENDED: Z.AI gives you Claude models!
Z_AI_API_KEY=your-z-ai-api-key
Z_AI_MODEL_DIAGNOSTIC=claude-3-5-sonnet-20241022
Z_AI_MODEL_PLANNER=claude-3-5-sonnet-20241022
Z_AI_MODEL_EXECUTOR=claude-3-sonnet-20240229
Z_AI_MODEL_VALIDATOR=claude-3-sonnet-20240229

# ===== QWEN/DASHSCOPE CONFIGURATION (if LLM_PROVIDER=qwen) =====
QWEN_API_KEY=your-qwen-dashscope-api-key
QWEN_MODEL_DIAGNOSTIC=qwen-max
QWEN_MODEL_PLANNER=qwen-max
QWEN_MODEL_EXECUTOR=qwen-turbo
QWEN_MODEL_VALIDATOR=qwen-max

# ===== HUGGING FACE CONFIGURATION (if LLM_PROVIDER=huggingface) =====
# Only needed if using local Hugging Face models
# NOTE: Claude is NOT available on Hugging Face (proprietary to Anthropic)
# Use these open-source models instead:
HF_TOKEN=your-huggingface-api-key
HF_DEVICE=cuda
HF_LOAD_IN_8BIT=true
HF_LOAD_IN_4BIT=false
HF_CACHE_DIR=./models

# Hugging Face Model Options:
# Option A: Mistral-based (RECOMMENDED for HF)
HF_MODEL_DIAGNOSTIC=mistralai/Mistral-7B-Instruct-v0.3
HF_MODEL_PLANNER=mistralai/Mistral-7B-Instruct-v0.3
HF_MODEL_EXECUTOR=mistralai/Mistral-7B-Instruct-v0.3
HF_MODEL_VALIDATOR=mistralai/Mistral-7B-Instruct-v0.3

# Option B: If you want smaller models (faster, less VRAM)
# Uncomment these instead:
# HF_MODEL_DIAGNOSTIC=microsoft/phi-2
# HF_MODEL_PLANNER=microsoft/phi-2
# HF_MODEL_EXECUTOR=mistralai/Mistral-7B-Instruct-v0.1
# HF_MODEL_VALIDATOR=microsoft/phi-2

# Option C: LLaMA 2 (if you have enough VRAM)
# Uncomment these instead:
# HF_MODEL_DIAGNOSTIC=meta-llama/Llama-2-7b-chat-hf
# HF_MODEL_PLANNER=meta-llama/Llama-2-7b-chat-hf
# HF_MODEL_EXECUTOR=meta-llama/Llama-2-7b-chat-hf
# HF_MODEL_VALIDATOR=meta-llama/Llama-2-7b-chat-hf

# ===== AGENT CONFIGURATION =====
ENVIRONMENT=production
LOG_LEVEL=INFO
MAX_RETRIES=3
APPROVAL_TIMEOUT_SECONDS=3600
PIPELINE_WAIT_TIMEOUT_SECONDS=1800

# ===== SECURITY =====
ENABLE_HUMAN_APPROVAL=true
ENABLE_AUDIT_LOGGING=true
ENABLE_RATE_LIMITING=true

# ===== INFRASTRUCTURE =====
KUBERNETES_NAMESPACE=lab-verse-monitoring
ENABLE_METRICS=true
METRICS_PORT=8001
