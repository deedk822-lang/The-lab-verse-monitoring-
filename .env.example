# Lab-Verse Agent - Configuration with Multi-Provider Support
# Copy this to .env.production and fill in the provider you want to use

# ===== PROVIDER SELECTION =====
# Choose which LLM provider to use:
# - "huggingface" (local models, requires download)
# - "z_ai" (Z.AI API)
# - "qwen" (Qwen/Alibaba Dashscope)
LLM_PROVIDER=z_ai

# ===== BITBUCKET CONFIGURATION =====
PIPELINE_PLATFORM=bitbucket
BITBUCKET_WORKSPACE=lab-verse-monitaring
BITBUCKET_USERNAME=your-email@atlassian.com
BITBUCKET_APP_PASSWORD=your-64-character-app-password

# ===== Z.AI CONFIGURATION (if LLM_PROVIDER=z_ai) =====
Z_AI_API_KEY=your-z-ai-api-key
Z_AI_MODEL_DIAGNOSTIC=claude-3-5-sonnet-20241022
Z_AI_MODEL_PLANNER=claude-3-5-sonnet-20241022
Z_AI_MODEL_EXECUTOR=claude-3-sonnet-20240229
Z_AI_MODEL_VALIDATOR=claude-3-sonnet-20240229

# ===== QWEN/DASHSCOPE CONFIGURATION (if LLM_PROVIDER=qwen) =====
QWEN_API_KEY=your-qwen-dashscope-api-key
QWEN_MODEL_DIAGNOSTIC=qwen-max
QWEN_MODEL_PLANNER=qwen-max
QWEN_MODEL_EXECUTOR=qwen-turbo
QWEN_MODEL_VALIDATOR=qwen-max

# ===== HUGGING FACE CONFIGURATION (if LLM_PROVIDER=huggingface) =====
# Only needed if using local Hugging Face models
HF_TOKEN=your-huggingface-api-key
HF_DEVICE=cuda
HF_LOAD_IN_8BIT=true
HF_LOAD_IN_4BIT=false
HF_CACHE_DIR=./models
HF_MODEL_DIAGNOSTIC=mistralai/Mistral-7B-Instruct-v0.3
HF_MODEL_PLANNER=microsoft/phi-2
HF_MODEL_EXECUTOR=TinyLlama/TinyLlama-1.1B-Chat-v1.0
HF_MODEL_VALIDATOR=mistralai/Mistral-7B-Instruct-v0.3

# ===== AGENT CONFIGURATION =====
ENVIRONMENT=production
LOG_LEVEL=INFO
MAX_RETRIES=3
APPROVAL_TIMEOUT_SECONDS=3600
PIPELINE_WAIT_TIMEOUT_SECONDS=1800

# ===== SECURITY =====
ENABLE_HUMAN_APPROVAL=true
ENABLE_AUDIT_LOGGING=true
ENABLE_RATE_LIMITING=true

# ===== INFRASTRUCTURE =====
KUBERNETES_NAMESPACE=lab-verse-monitoring
ENABLE_METRICS=true
METRICS_PORT=8001
