name: LLM-Powered Code Review (FIXED)

on:
  pull_request:
    types: [opened, synchronize]
  workflow_dispatch:

permissions:
  contents: write
  pull-requests: write

jobs:
  # ========================================================================
  # STEP 1: Collect Findings
  # ========================================================================

  dependency-review:
    name: Dependency Review
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    steps:
      - name: Checkout
        uses: actions/checkout@v4
      - name: Dependency Review
        uses: actions/dependency-review-action@v4
        with:
          fail-on-severity: moderate
          allow-licenses: MIT, Apache-2.0, BSD-3-Clause

  collect-findings:
    name: Collect Code Review Findings
    runs-on: ubuntu-latest
    outputs:
      findings_file: ${{ steps.collect.outputs.findings_file }}

    steps:
      - uses: actions/checkout@v4
        with:
          submodules: 'recursive'
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install analysis tools
        run: |
          pip install bandit ruff mypy safety

      - name: Run static analysis
        id: collect
        continue-on-error: true
        run: |
          mkdir -p analysis-results

          bandit -r src/ -f json -o analysis-results/bandit.json || echo "bandit failed"
          ruff check src/ --output-format=json > analysis-results/ruff.json || echo "ruff failed"
          mypy src/ --json-report analysis-results/ || echo "mypy failed"
          safety check > analysis-results/safety.txt || echo "safety failed"

          # Convert safety text output to JSON
          python3 << 'PY'
          import json
          import re
          import os

          findings = []

          try:
              if os.path.exists('analysis-results/safety.txt'):
                  with open('analysis-results/safety.txt', 'r') as f:
                      content = f.read()

                  # Parse safety output format
                  # Example: "requests 2.28.1 (CVE-2023-32681) - High severity"
                  pattern = r'(\S+)\s+([\d.]+)\s+\(([^)]+)\)'

                  for line in content.split('\n'):
                      match = re.search(pattern, line)
                      if match:
                          findings.append({
                              'file': 'requirements.txt',
                              'line': 0,
                              'severity': 'high' if 'high' in line.lower() else 'medium',
                              'issue': f"Vulnerability in {match.group(1)} {match.group(2)}: {match.group(3)}",
                              'suggestion': 'Update dependency'
                          })

              # Write as JSON
              with open('analysis-results/safety.json', 'w') as f:
                  json.dump(findings, f, indent=2)

              print(f"‚úÖ Converted {len(findings)} safety findings to JSON")

          except Exception as e:
              print(f"‚ö†Ô∏è Safety parse error: {e}")
              # Create empty JSON to prevent crash
              with open('analysis-results/safety.json', 'w') as f:
                  json.dump([], f)
          PY

          echo "findings_file=analysis-results" >> $GITHUB_OUTPUT

      - name: Upload findings
        uses: actions/upload-artifact@v4
        with:
          name: analysis-results
          path: analysis-results/
          retention-days: 7

  # ========================================================================
  # STEP 2: Reasoning + Coding + Testing (Combined)
  # ========================================================================

  process-and-test:
    name: Process Findings and Test
    runs-on: ubuntu-latest
    needs: [collect-findings]
    outputs:
      tests_passed: ${{ steps.test.outputs.tests_passed }}
      test_exit_code: ${{ steps.test.outputs.test_exit_code }}

    steps:
      - uses: actions/checkout@v4
        with:
          submodules: 'recursive'
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install package
        run: |
          pip install -e ".[dev]"

      - name: Download findings
        uses: actions/download-artifact@v4
        with:
          name: analysis-results
          path: analysis-results/

      - name: üîí Setup network restrictions
        run: |
          # Block metadata service (defense in depth)
          sudo iptables -A OUTPUT -d 169.254.169.254 -j DROP
          # Block internal GitHub network ranges
          sudo iptables -A OUTPUT -d 10.0.0.0/8,172.16.0.0/12,192.168.0.0/16 -j DROP

      - name: Install and start Ollama
        timeout-minutes: 10
        run: |
          curl -fsSL https://ollama.com/install.sh | sh
          nohup ollama serve > /tmp/ollama.log 2>&1 &
          sleep 10
          # Use smaller models for standard runners
          ollama pull deepseek-r1:1.5b &
          ollama pull qwen2.5-coder:1.5b &
          wait

      - name: Run orchestrator (review)
        timeout-minutes: 10
        run: |
          # Pre-validate findings size
          FINDINGS_SIZE=$(wc -c < analysis-results/safety.json 2>/dev/null || echo 0)

          if [ $FINDINGS_SIZE -gt 50000 ]; then
            echo "‚ö†Ô∏è Findings too large (${FINDINGS_SIZE} bytes), truncating..."
            python3 << 'PY'
            import json
            try:
                with open('analysis-results/safety.json', 'r') as f:
                    data = json.load(f)
                # Keep only first 20 findings
                if isinstance(data, list):
                    data = data[:20]
                elif isinstance(data, dict) and 'findings' in data:
                    data['findings'] = data['findings'][:20]
                with open('analysis-results/safety.json', 'w') as f:
                    json.dump(data, f)
            except:
                pass
            PY
          fi

          # Create directory for proposals
          mkdir -p artifacts/proposals

          # Run with timeout protection
          timeout 300 python src/pr_fix_agent/orchestrator.py \
            review \
            --findings analysis-results/safety.json \
            --limit 5 || echo "Analysis timed out but continuing..."

          # Move proposals to expected location
          if [ -f proposals.json ]; then
            mv proposals.json artifacts/proposals/proposals.json
          else
            echo "[]" > artifacts/proposals/proposals.json
          fi

      - name: Run tests (FIXED LOGIC)
        id: test
        run: |
          set +e

          mkdir -p artifacts/test-results
          pytest tests/ -v \
            --json-report \
            --json-report-file=artifacts/test-results/test-results.json \
            --tb=short

          TEST_EXIT_CODE=$?
          set -e

          echo "test_exit_code=$TEST_EXIT_CODE" >> $GITHUB_OUTPUT

          if [ $TEST_EXIT_CODE -eq 0 ] && [ -f artifacts/test-results/test-results.json ]; then
            echo "tests_passed=true" >> $GITHUB_OUTPUT
            echo "‚úÖ All tests passed (exit code: 0)"
          else
            echo "tests_passed=false" >> $GITHUB_OUTPUT
            echo "‚ùå Tests failed (exit code: $TEST_EXIT_CODE)"
          fi

      - name: Upload proposals
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: fix-proposals
          path: artifacts/proposals/proposals.json
          retention-days: 7

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results
          path: artifacts/test-results/test-results.json
          retention-days: 30

      - name: Upload fixed code
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: fixed-code
          path: src/
          retention-days: 7

  # ========================================================================
  # STEP 3: Create PR (Only if tests pass)
  # ========================================================================

  create-pr:
    name: Create Pull Request
    runs-on: ubuntu-latest
    needs: [process-and-test]
    if: needs.process-and-test.outputs.tests_passed == 'true'

    steps:
      - uses: actions/checkout@v4
        with:
          submodules: 'recursive'
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install package
        run: pip install -e .

      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/

      - name: Generate PR body (FIXED)
        run: |
          python -m pr_fix_agent.orchestrator \
            --mode generate-pr \
            --proposals artifacts/fix-proposals/proposals.json \
            --test-results artifacts/test-results/test-results.json \
            --output pr-body.md

      - name: Apply fixed code
        run: |
          cp -r artifacts/fixed-code/* src/

      - name: Create Pull Request
        uses: peter-evans/create-pull-request@v7
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          commit-message: "ü§ñ Auto-fix: Code review findings"
          title: "ü§ñ Auto-fix: Code review findings"
          body-path: pr-body.md
          branch: auto-fix/code-review-${{ github.run_number }}
          base: ${{ github.event.pull_request.base.ref || 'main' }}
          labels: |
            automated-fix
            code-review
          draft: false
          delete-branch: true

  # ========================================================================
  # STEP 4: Create Draft PR (If tests fail)
  # ========================================================================

  create-draft-pr:
    name: Create Draft PR (Tests Failed)
    runs-on: ubuntu-latest
    needs: [process-and-test]
    if: needs.process-and-test.outputs.tests_passed == 'false'

    steps:
      - uses: actions/checkout@v4
        with:
          submodules: 'recursive'
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install package
        run: pip install -e .

      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/

      - name: Generate PR body with test failures
        run: |
          python -m pr_fix_agent.orchestrator \
            --mode generate-pr \
            --proposals artifacts/fix-proposals/proposals.json \
            --test-results artifacts/test-results/test-results.json \
            --output pr-body.md

      - name: Apply fixed code
        run: |
          cp -r artifacts/fixed-code/* src/ || echo "No fixed code to apply"

      - name: Create Draft Pull Request
        uses: peter-evans/create-pull-request@v7
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          commit-message: "ü§ñ Auto-fix: Code review findings (TESTS FAILED)"
          title: "ü§ñ [DRAFT] Auto-fix: Code review findings ‚ùå Tests Failed"
          body-path: pr-body.md
          branch: auto-fix/code-review-failed-${{ github.run_number }}
          base: ${{ github.event.pull_request.base.ref || 'main' }}
          labels: |
            automated-fix
            code-review
            tests-failed
          draft: true
          delete-branch: false

  # ========================================================================
  # STEP 5: Summary
  # ========================================================================

  report-summary:
    name: Report Summary
    runs-on: ubuntu-latest
    needs: [process-and-test, create-pr, create-draft-pr]
    if: always()

    steps:
      - name: Generate summary
        run: |
          echo "## ü§ñ LLM Code Review Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ needs.process-and-test.outputs.tests_passed }}" == "true" ]; then
            echo "- **Tests**: ‚úÖ PASSED (exit code: ${{ needs.process-and-test.outputs.test_exit_code }})" >> $GITHUB_STEP_SUMMARY
            echo "- **PR Created**: ${{ needs.create-pr.result == 'success' && '‚úÖ Regular PR' || '‚ö†Ô∏è Failed' }}" >> $GITHUB_STEP_SUMMARY
          else
            echo "- **Tests**: ‚ùå FAILED (exit code: ${{ needs.process-and-test.outputs.test_exit_code }})" >> $GITHUB_STEP_SUMMARY
            echo "- **PR Created**: ${{ needs.create-draft-pr.result == 'success' && '‚ö†Ô∏è Draft PR (needs review)' || '‚ùå Failed' }}" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Details" >> $GITHUB_STEP_SUMMARY
          echo "- Exit code indicates actual test outcome" >> $GITHUB_STEP_SUMMARY
          echo "- Draft PR created if tests fail" >> $GITHUB_STEP_SUMMARY
          echo "- Regular PR created if tests pass" >> $GITHUB_STEP_SUMMARY
