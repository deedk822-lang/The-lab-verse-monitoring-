name: LLM-Powered Code Review (FIXED)

on:
  pull_request:
    types: [opened, synchronize]
  workflow_dispatch:

permissions:
  contents: write
  pull-requests: write

env:
  # Resource constraints to prevent OOM
  OLLAMA_TIMEOUT: 600
  MAX_MEMORY_MB: 6144

jobs:
  # ========================================================================
  # STEP 1: Collect Findings
  # ========================================================================

  collect-findings:
    name: Collect Code Review Findings
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      findings_file: ${{ steps.collect.outputs.findings_file }}

    steps:
      - uses: actions/checkout@v4
        with:
          submodules: 'recursive'
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install analysis tools
        run: |
          pip install bandit ruff mypy safety --quiet

      - name: Run static analysis
        id: collect
        continue-on-error: true
        run: |
          mkdir -p analysis-results

          bandit -r src/pr_fix_agent -f json -o analysis-results/bandit.json || echo "bandit failed"
          ruff check src/pr_fix_agent --output-format=json > analysis-results/ruff.json || echo "ruff failed"
          mypy src/pr_fix_agent --ignore-missing-imports --json-report analysis-results/ || echo "mypy failed"
          safety check --json > analysis-results/safety.json || echo "safety failed"

          echo "findings_file=analysis-results" >> $GITHUB_OUTPUT

      - name: Upload findings
        uses: actions/upload-artifact@v4
        with:
          name: analysis-results
          path: analysis-results/
          retention-days: 7

  # ========================================================================
  # STEP 2: Reasoning + Coding + Testing (Combined)
  # ========================================================================

  process-and-test:
    name: Process Findings and Test
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [collect-findings]
    outputs:
      tests_passed: ${{ steps.test.outputs.tests_passed }}
      test_exit_code: ${{ steps.test.outputs.test_exit_code }}

    steps:
      - uses: actions/checkout@v4
        with:
          submodules: 'recursive'
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install package
        run: |
          pip install -e ".[dev]" --quiet

      - name: Download findings
        uses: actions/download-artifact@v4
        with:
          name: analysis-results
          path: analysis-results/

      - name: Install and start Ollama
        timeout-minutes: 10
        run: |
          curl -fsSL https://ollama.com/install.sh | sh
          ollama serve &
          sleep 10
          # Use smaller models for standard runners (memory-constrained)
          ollama pull deepseek-r1:1.5b || echo "Model pull may have timed out, continuing..."
          ollama pull qwen2.5-coder:1.5b || echo "Model pull may have timed out, continuing..."

      - name: Run orchestrator (reasoning + coding)
        timeout-minutes: 8
        run: |
          # Create directory for proposals
          mkdir -p artifacts/proposals

          # Check if Ollama is running
          if ! curl -s http://localhost:11434/api/tags > /dev/null; then
            echo "âš ï¸ Ollama not available, skipping LLM processing"
            echo '[]' > artifacts/proposals/proposals.json
            exit 0
          fi

          python -m pr_fix_agent.orchestrator \
            review \
            --findings analysis-results/ \
            --backend ollama || echo "Orchestrator may have failed, continuing..."

          # Copy proposals if they exist
          if [ -f proposals.json ]; then
            cp proposals.json artifacts/proposals/
          else
            echo '[]' > artifacts/proposals/proposals.json
          fi

      - name: Run tests (FIXED LOGIC)
        id: test
        timeout-minutes: 5
        run: |
          set +e

          mkdir -p artifacts/test-results

          # Only run tests if tests directory exists
          if [ -d tests ]; then
            pytest tests/ -v \
              --json-report \
              --json-report-file=artifacts/test-results/test-results.json \
              --tb=short 2>/dev/null || true
          fi

          # Check if test results exist
          if [ -f artifacts/test-results/test-results.json ]; then
            TEST_EXIT_CODE=0
            # Check if any tests failed
            if python3 -c "import json; data=json.load(open('artifacts/test-results/test-results.json')); exit(0 if data.get('summary', {}).get('failed', 0) == 0 else 1)" 2>/dev/null; then
              echo "tests_passed=true" >> $GITHUB_OUTPUT
              echo "âœ… All tests passed"
            else
              echo "tests_passed=false" >> $GITHUB_OUTPUT
              echo "âŒ Some tests failed"
              TEST_EXIT_CODE=1
            fi
          else
            # No tests found, consider it passed
            echo "tests_passed=true" >> $GITHUB_OUTPUT
            echo "test_exit_code=0" >> $GITHUB_OUTPUT
            echo "âœ… No tests to run"
            exit 0
          fi

          echo "test_exit_code=$TEST_EXIT_CODE" >> $GITHUB_OUTPUT
          set -e

      - name: Upload proposals
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: fix-proposals
          path: artifacts/proposals/proposals.json
          retention-days: 7

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results
          path: artifacts/test-results/test-results.json
          retention-days: 30

  # ========================================================================
  # STEP 3: Create PR (Only if tests pass)
  # ========================================================================

  create-pr:
    name: Create Pull Request
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: [process-and-test]
    if: needs.process-and-test.outputs.tests_passed == 'true'

    steps:
      - uses: actions/checkout@v4
        with:
          submodules: 'recursive'
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install package
        run: pip install -e . --quiet

      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/

      - name: Generate PR body (FIXED)
        run: |
          if [ -f artifacts/fix-proposals/proposals.json ]; then
            python -m pr_fix_agent.orchestrator \
              generate-pr \
              --findings artifacts/fix-proposals/proposals.json || echo "Using default PR body"
          fi

          # Create default PR body if needed
          if [ ! -f pr-body.md ]; then
            echo "# ðŸ¤– Automated Code Review Fixes" > pr-body.md
            echo "" >> pr-body.md
            echo "Tests: âœ… PASSED" >> pr-body.md
          fi

      - name: Create Pull Request
        uses: peter-evans/create-pull-request@v7
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          commit-message: "ðŸ¤– Auto-fix: Code review findings"
          title: "ðŸ¤– Auto-fix: Code review findings"
          body-path: pr-body.md
          branch: auto-fix/code-review-${{ github.run_number }}
          base: ${{ github.event.pull_request.base.ref || 'main' }}
          labels: |
            automated-fix
            code-review
          draft: false
          delete-branch: true

  # ========================================================================
  # STEP 4: Create Draft PR (If tests fail)
  # ========================================================================

  create-draft-pr:
    name: Create Draft PR (Tests Failed)
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: [process-and-test]
    if: needs.process-and-test.outputs.tests_passed == 'false'

    steps:
      - uses: actions/checkout@v4
        with:
          submodules: 'recursive'
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install package
        run: pip install -e . --quiet

      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts/

      - name: Generate PR body with test failures
        run: |
          echo "# ðŸ¤– Automated Code Review Fixes" > pr-body.md
          echo "" >> pr-body.md
          echo "Tests: âŒ FAILED" >> pr-body.md
          echo "" >> pr-body.md
          echo "This PR needs manual review." >> pr-body.md

      - name: Create Draft Pull Request
        uses: peter-evans/create-pull-request@v7
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          commit-message: "ðŸ¤– Auto-fix: Code review findings (TESTS FAILED)"
          title: "ðŸ¤– [DRAFT] Auto-fix: Code review findings âŒ Tests Failed"
          body-path: pr-body.md
          branch: auto-fix/code-review-failed-${{ github.run_number }}
          base: ${{ github.event.pull_request.base.ref || 'main' }}
          labels: |
            automated-fix
            code-review
            tests-failed
          draft: true
          delete-branch: false

  # ========================================================================
  # STEP 5: Summary
  # ========================================================================

  report-summary:
    name: Report Summary
    runs-on: ubuntu-latest
    timeout-minutes: 2
    needs: [process-and-test, create-pr, create-draft-pr]
    if: always()

    steps:
      - name: Generate summary
        run: |
          echo "## ðŸ¤– LLM Code Review Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ needs.process-and-test.outputs.tests_passed }}" == "true" ]; then
            echo "- **Tests**: âœ… PASSED" >> $GITHUB_STEP_SUMMARY
            echo "- **PR Created**: ${{ needs.create-pr.result == 'success' && 'âœ… Regular PR' || 'âš ï¸ Failed' }}" >> $GITHUB_STEP_SUMMARY
          else
            echo "- **Tests**: âŒ FAILED" >> $GITHUB_STEP_SUMMARY
            echo "- **PR Created**: ${{ needs.create-draft-pr.result == 'success' && 'âš ï¸ Draft PR (needs review)' || 'âŒ Failed' }}" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Details" >> $GITHUB_STEP_SUMMARY
          echo "- Exit code indicates actual test outcome" >> $GITHUB_STEP_SUMMARY
          echo "- Draft PR created if tests fail" >> $GITHUB_STEP_SUMMARY
          echo "- Regular PR created if tests pass" >> $GITHUB_STEP_SUMMARY
