name: ðŸ¤– PR Fix Agent (Local Ollama)

on:
  pull_request:
    types: [opened, synchronize]
    branches: [main, develop]
  workflow_call:

jobs:
  analyze-and-fix:
    runs-on: ubuntu-latest
    permissions:
      contents: write
      pull-requests: write
      checks: write

    steps:
      # ==========================================
      # 1. Setup Environment
      # ==========================================
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      # ==========================================
      # 2. Install Ollama (Local LLM - FREE)
      # ==========================================
      - name: ðŸ¦™ Install Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh
          nohup ollama serve > /tmp/ollama.log 2>&1 &
          sleep 5
          ollama --version

      - name: ðŸ§  Pull Local Models (Cache Optimized)
        run: |
          # Pull lightweight models for different tasks
          ollama pull codellama:7b &
          ollama pull mistral:7b &
          ollama pull llama3.2 &
          wait
          ollama list

      # ==========================================
      # 3. Install Project Dependencies
      # ==========================================
      - name: ðŸ“¦ Install Dependencies
        run: |
          pip install -e ".[dev]" || pip install -r requirements.txt
          pip install ollama pytest bandit ruff mypy safety

      # ==========================================
      # 4. Run CI Checks & Capture Errors
      # ==========================================
      - name: ðŸ” Run Type Checker (mypy)
        id: typecheck
        continue-on-error: true
        run: |
          mypy src/ --ignore-missing-imports --show-error-codes 2>&1 | tee type_errors.txt || true
          echo "TYPE_ERRORS=$(wc -l < type_errors.txt)" >> $GITHUB_ENV

      - name: ðŸ”’ Run Security Scan (bandit)
        id: security
        continue-on-error: true
        run: |
          bandit -r src/ -f txt -o security_errors.txt || true
          echo "SECURITY_ERRORS=$(wc -l < security_errors.txt)" >> $GITHUB_ENV

      - name: ðŸ§¹ Run Linter (ruff)
        id: lint
        continue-on-error: true
        run: |
          ruff check src/ 2>&1 | tee lint_errors.txt || true
          echo "LINT_ERRORS=$(wc -l < lint_errors.txt)" >> $GITHUB_ENV

      - name: ðŸ§ª Run Tests
        id: tests
        continue-on-error: true
        run: |
          pytest tests/ -v --tb=short 2>&1 | tee test_errors.txt || true
          echo "TEST_ERRORS=$(wc -l < test_errors.txt)" >> $GITHUB_ENV

      # ==========================================
      # 5. AI Analysis with Local Models
      # ==========================================
      - name: ðŸ¤– Analyze Errors with Ollama
        id: analysis
        run: |
          cat > analyze_errors.py << 'SCRIPT'
          import ollama
          import json
          import sys

          def analyze_with_model(model, error_file, error_type):
              """Analyze errors using local Ollama model"""
              try:
                  with open(error_file, 'r') as f:
                      errors = f.read()[:4000]  # Limit context

                  if not errors or len(errors) < 10:
                      return None

                  prompt = f"""You are a Python expert. Fix these {error_type} errors:

{errors}

Provide:
1. Root cause analysis (2 sentences)
2. Specific fixes with file paths and code changes
3. Shell commands to apply fixes

Format as JSON:
{{"fixes": [{{"file": "path", "action": "description", "command": "shell command"}}]}}"""

                  response = ollama.chat(
                      model=model,
                      messages=[{'role': 'user', 'content': prompt}],
                      options={'temperature': 0.1, 'num_ctx': 4096}
                  )

                  return response['message']['content']
              except Exception as e:
                  return f"Analysis failed: {str(e)}"

          # Analyze different error types with appropriate models
          results = {}

          # Type errors -> CodeLlama (code specialist)
          if ${{ env.TYPE_ERRORS }} > 0:
              print("ðŸ” Analyzing type errors with CodeLlama...")
              results['type_fixes'] = analyze_with_model('codellama:7b', 'type_errors.txt', 'type checking (mypy)')

          # Security -> Mistral (reasoning)
          if ${{ env.SECURITY_ERRORS }} > 0:
              print("ðŸ”’ Analyzing security issues with Mistral...")
              results['security_fixes'] = analyze_with_model('mistral:7b', 'security_errors.txt', 'security (bandit)')

          # Lint errors -> Llama 3.2 (fast)
          if ${{ env.LINT_ERRORS }} > 0:
              print("ðŸ§¹ Analyzing lint errors with Llama 3.2...")
              results['lint_fixes'] = analyze_with_model('llama3.2', 'lint_errors.txt', 'linting (ruff)')

          # Write results
          with open('ai_fixes.json', 'w') as f:
              json.dump(results, f, indent=2)

          print("âœ… Analysis complete")
          SCRIPT

          python analyze_errors.py

      # ==========================================
      # 6. Auto-Apply Safe Fixes
      # ==========================================
      - name: ðŸ”§ Auto-Fix with Ruff (Safe Mode)
        if: env.LINT_ERRORS > 0
        run: |
          # Auto-fix what we can safely
          ruff check src/ tests/ --fix --exit-zero
          ruff format src/ tests/

          # Check if changes were made
          if [ -n "$(git status --porcelain)" ]; then
            echo "CHANGES_MADE=true" >> $GITHUB_ENV
          fi

      - name: ðŸ”§ Auto-Fix Import Issues
        if: env.TYPE_ERRORS > 0
        run: |
          # Common Python import fixes using Ollama guidance
          python << 'FIX'
          import ollama
          import re

          # Read type errors
          try:
              with open('type_errors.txt', 'r') as f:
                  errors = f.read()

              # Extract missing imports
              missing = re.findall(r'"([^"]+)" is not defined', errors)

              if missing:
                  print(f"Found missing imports: {missing}")

                  # Ask Ollama for import suggestions
                  response = ollama.chat(
                      model='codellama:7b',
                      messages=[{
                          'role': 'user',
                          'content': f'What are the correct import statements for: {", ".join(missing)}? Return only the import lines.'
                      }]
                  )

                  print("Suggested imports:", response['message']['content'])
          except Exception as e:
              print(f"Auto-fix error: {e}")
          FIX

      # ==========================================
      # 7. Commit Fixes Back to PR
      # ==========================================
      - name: ðŸ’¾ Commit Auto-Fixes
        if: env.CHANGES_MADE == 'true'
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "ðŸ¤– PR Fix Agent"
          git add .
          git commit -m "ðŸ¤– Auto-fix: Ruff formatting and lint issues" || echo "No changes to commit"
          git push

      # ==========================================
      # 8. Generate Report
      # ==========================================
      - name: ðŸ“ Generate PR Comment
        if: github.event.pull_request
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            let report = `## ðŸ¤– PR Fix Agent Report

            **Models Used**: \`codellama:7b\` (code), \`mistral:7b\` (security), \`llama3.2\` (lint) - **Zero API costs**

            ### Check Results
            `;

            // Add results for each check
            const typeErrors = process.env.TYPE_ERRORS || 0;
            const secErrors = process.env.SECURITY_ERRORS || 0;
            const lintErrors = process.env.LINT_ERRORS || 0;
            const testErrors = process.env.TEST_ERRORS || 0;

            report += `- ${typeErrors > 0 ? 'âŒ' : 'âœ…'} Type Check (mypi): ${typeErrors} issues\n`;
            report += `- ${secErrors > 0 ? 'âŒ' : 'âœ…'} Security (bandit): ${secErrors} issues\n`;
            report += `- ${lintErrors > 0 ? 'âš ï¸' : 'âœ…'} Lint (ruff): ${lintErrors} issues (auto-fixed if possible)\n`;
            report += `- ${testErrors > 0 ? 'âŒ' : 'âœ…'} Tests: ${testErrors} issues\n`;

            // Add AI suggestions if available
            try {
              const fixes = JSON.parse(fs.readFileSync('ai_fixes.json', 'utf8'));
              if (Object.keys(fixes).length > 0) {
                report += `\n### ðŸ’¡ AI Suggestions (Powered by Ollama)\n`;
                for (const [category, suggestion] of Object.entries(fixes)) {
                  if (suggestion) {
                    report += `<details><summary>${category}</summary>\n\n\`\`\`\n${suggestion}\n\`\`\`\n</details>\n\n`;
                  }
                }
              }
            } catch (e) {
              console.log('No AI fixes file found');
            }

            report += `\n---\nðŸ¦™ Using local Ollama models to save API credits. Manual review recommended for complex fixes.`;

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });
