// pages/api/mcp/huggingface/messages.js
/**
 * HuggingFace MCP Gateway Endpoint
 * Receives tool calls from MCP server and executes them against HF API
 */

export default async function handler(req, res) {
  // 1. Authentication
  const authHeader = req.headers.authorization;
  const token = authHeader?.replace('Bearer ', '');
  
  const validToken = token === process.env.GATEWAY_API_KEY;
  
  if (!validToken) {
    return res.status(401).json({ error: 'Unauthorized' });
  }

  // 2. Validate request
  if (req.method !== 'POST') {
    return res.status(405).json({ error: 'Method not allowed' });
  }

  const { messages } = req.body;
  if (!messages || !Array.isArray(messages)) {
    return res.status(400).json({ error: 'Invalid request format' });
  }

  try {
    // 3. Parse tool call from message content
    const content = messages[messages.length - 1]?.content || '';
    let toolCall;
    try {
      toolCall = JSON.parse(content);
    } catch (e) {
      return res.status(400).json({ 
        error: 'Invalid tool call format. Content must be a valid JSON string.',
        details: e.message 
      });
    }

    const { tool: toolName, args } = toolCall;
    if (!toolName || !args) {
      return res.status(400).json({ error: 'Tool call must include "tool" and "args" keys.' });
    }

    // 4. Execute tool based on name
    let result;
    const HF_TOKEN = process.env.HF_API_TOKEN;

    switch (toolName) {
      case 'hf_list_models':
        result = await listModels(args.search, args.limit, HF_TOKEN);
        break;
      
      case 'hf_model_info':
        result = await getModelInfo(args.model, HF_TOKEN);
        break;
      
      case 'hf_list_datasets':
        result = await listDatasets(args.search, args.limit, HF_TOKEN);
        break;
      
      case 'hf_list_spaces':
        result = await listSpaces(args.search, args.limit, HF_TOKEN);
        break;
      
      case 'hf_run_inference':
        result = await runInference(args.model, args.inputs, args.parameters, HF_TOKEN);
        break;
      
      default:
        return res.status(400).json({ 
          error: 'Unknown tool',
          tool: toolName 
        });
    }

    // 5. Return in OpenAI-compatible format
    return res.status(200).json({
      id: `chatcmpl-${Date.now()}`,
      object: 'chat.completion',
      created: Math.floor(Date.now() / 1000),
      model: 'hf-mcp',
      choices: [{
        index: 0,
        message: {
          role: 'assistant',
          content: JSON.stringify(result, null, 2)
        },
        finish_reason: 'stop'
      }],
      usage: {
        prompt_tokens: 0,
        completion_tokens: 0,
        total_tokens: 0
      }
    });

  } catch (error) {
    console.error('Gateway error:', error);
    return res.status(500).json({
      error: 'Internal server error',
      message: error.message
    });
  }
}

// ==================== HuggingFace API Functions ====================

async function listModels(search = '', limit = 10, token) {
  const url = new URL('https://huggingface.co/api/models');
  if (search) url.searchParams.set('search', search);
  url.searchParams.set('limit', limit);
  
  const response = await fetch(url, {
    headers: token ? { 'Authorization': `Bearer ${token}` } : {}
  });
  
  if (!response.ok) {
    throw new Error(`HuggingFace API error: ${response.status}`);
  }
  
  const models = await response.json();
  return {
    total: models.length,
    models: models.map(m => ({
      id: m.id,
      author: m.author,
      downloads: m.downloads,
      likes: m.likes,
      tags: m.tags,
      pipeline_tag: m.pipeline_tag
    }))
  };
}

async function getModelInfo(modelId, token) {
  const response = await fetch(`https://huggingface.co/api/models/${modelId}`, {
    headers: token ? { 'Authorization': `Bearer ${token}` } : {}
  });
  
  if (!response.ok) {
    throw new Error(`Model not found: ${modelId}`);
  }
  
  const model = await response.json();
  return {
    id: model.id,
    author: model.author,
    downloads: model.downloads,
    likes: model.likes,
    tags: model.tags,
    pipeline_tag: model.pipeline_tag,
    library_name: model.library_name,
    created_at: model.created_at,
    last_modified: model.lastModified,
    siblings: model.siblings?.length || 0,
    private: model.private,
    disabled: model.disabled
  };
}

async function listDatasets(search = '', limit = 10, token) {
  const url = new URL('https://huggingface.co/api/datasets');
  if (search) url.searchParams.set('search', search);
  url.searchParams.set('limit', limit);
  
  const response = await fetch(url, {
    headers: token ? { 'Authorization': `Bearer ${token}` } : {}
  });
  
  if (!response.ok) {
    throw new Error(`HuggingFace API error: ${response.status}`);
  }
  
  const datasets = await response.json();
  return {
    total: datasets.length,
    datasets: datasets.map(d => ({
      id: d.id,
      author: d.author,
      downloads: d.downloads,
      likes: d.likes,
      tags: d.tags
    }))
  };
}

async function listSpaces(search = '', limit = 10, token) {
  const url = new URL('https://huggingface.co/api/spaces');
  if (search) url.searchParams.set('search', search);
  url.searchParams.set('limit', limit);
  
  const response = await fetch(url, {
    headers: token ? { 'Authorization': `Bearer ${token}` } : {}
  });
  
  if (!response.ok) {
    throw new Error(`HuggingFace API error: ${response.status}`);
  }
  
  const spaces = await response.json();
  return {
    total: spaces.length,
    spaces: spaces.map(s => ({
      id: s.id,
      author: s.author,
      likes: s.likes,
      sdk: s.sdk,
      tags: s.tags
    }))
  };
}

async function runInference(modelId, inputs, parameters = {}, token) {
  if (!token) {
    throw new Error('HF_API_TOKEN required for inference');
  }
  
  const response = await fetch(
    `https://api-inference.huggingface.co/models/${modelId}`,
    {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${token}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        inputs,
        parameters
      })
    }
  );
  
  if (!response.ok) {
    const error = await response.text();
    throw new Error(`Inference failed: ${error}`);
  }
  
  const result = await response.json();
  return {
    model: modelId,
    inputs,
    result
  };
}
