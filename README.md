# Hybrid Brain-to-Hands Integration: Production Architecture

## Executive Summary

This integration transforms passive monitoring into an **Autonomous Builder System** by combining cloud-based AI reasoning (Kimi - "The Brain") with local execution infrastructure (FileSystemAgent - "The Hands"). The result is a closed-loop engineering cycle that maintains code quality, security, and governance while enabling autonomous development.

**Key Metrics:**
- Context Window: 128k tokens for full repository analysis
- Confidence Threshold: 85% for auto-merge eligibility
- Security Gates: Multi-layer validation (Semgrep, tests, lint, audit)
- Data Sovereignty: Code generation in cloud, execution behind firewall

---

## Architecture Overview

### The Three-Layer Model

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     ORCHESTRATION LAYER                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚   Intent   â”‚â”€â”€â”€â”€â”€â–¶â”‚ Task Router  â”‚â”€â”€â”€â”€â”€â–¶â”‚  Validator  â”‚ â”‚
â”‚  â”‚  Manager   â”‚      â”‚ (route_task) â”‚      â”‚   Engine    â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     REASONING LAYER (Cloud)                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  KIMI AI Engine (128k context)                         â”‚ â”‚
â”‚  â”‚  â€¢ Full repository context analysis                    â”‚ â”‚
â”‚  â”‚  â€¢ Multi-file change orchestration                     â”‚ â”‚
â”‚  â”‚  â€¢ Test generation & validation logic                  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   EXECUTION LAYER (Local)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ FileSystem   â”‚  â”‚  Security    â”‚  â”‚    Deploy        â”‚  â”‚
â”‚  â”‚   Agent      â”‚â”€â–¶â”‚   Gates      â”‚â”€â–¶â”‚   Pipeline       â”‚  â”‚
â”‚  â”‚ (fs_agent)   â”‚  â”‚  (Semgrep)   â”‚  â”‚ (deploy.sh)      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Critical Design Principles

1. **Zero Trust Execution**: All generated code passes through local validation before deployment
2. **Audit Everything**: Comprehensive logging of all AI-generated changes
3. **Progressive Confidence**: Confidence scoring determines automation level
4. **Human-in-Loop**: Critical paths always require human approval
5. **Fail-Safe Defaults**: System defaults to manual review on uncertainty

---

## Production Implementation

### 1. Task Router (Node.js)

```javascript
// route_task.js - Production-ready task orchestration
const fs = require('fs').promises;
const path = require('path');
const { KimiClient } = require('./clients/kimi');
const { FileSystemAgent } = require('./agents/fs_agent');
const { ConfidenceScorer } = require('./scoring/confidence');
const { AuditLogger } = require('./logging/audit');

class TaskRouter {
  constructor(config) {
    this.kimi = new KimiClient(config.kimi);
    this.fsAgent = new FileSystemAgent(config.fsAgent);
    this.scorer = new ConfidenceScorer(config.scoring);
    this.audit = new AuditLogger(config.audit);
    this.protectedPaths = config.protectedPaths || [
      /^\.github\/workflows/,
      /^config\//,
      /^setup\./,
      /docker-compose\.yml$/,
      /package\.json$/,
      /requirements\.txt$/
    ];
  }

  async routeTask(request) {
    const taskId = this.generateTaskId();
    const context = {
      taskId,
      intent: request.intent,
      timestamp: new Date().toISOString(),
      requestor: request.requestor || 'system'
    };

    try {
      // Step 1: Log task initiation
      await this.audit.logTaskStart(context);

      // Step 2: Fetch repository context
      const repoContext = await this.fetchRepoContext(request.scope);

      // Step 3: Send to Kimi for reasoning
      console.log(`[${taskId}] Sending to Kimi Brain...`);
      const kimiResponse = await this.kimi.generate({
        intent: request.intent,
        context: repoContext,
        constraints: this.buildConstraints(request)
      });

      // Step 4: Validate response format
      const changes = this.validateKimiResponse(kimiResponse);

      // Step 5: Check for protected paths
      const protectedChanges = this.checkProtectedPaths(changes);
      if (protectedChanges.length > 0) {
        await this.audit.logProtectedPathAccess(taskId, protectedChanges);
        return {
          success: false,
          reason: 'protected_paths',
          requiresHumanReview: true,
          protectedFiles: protectedChanges,
          taskId
        };
      }

      // Step 6: Calculate initial confidence
      const confidence = await this.scorer.calculateConfidence({
        changes,
        intent: request.intent,
        kimiMetadata: kimiResponse.metadata
      });

      // Step 7: Apply changes to sandbox
      const sandboxBranch = `bot/task-${taskId}`;
      await this.fsAgent.createBranch(sandboxBranch);

      console.log(`[${taskId}] Applying ${Object.keys(changes).length} file changes...`);
      const applyResults = await this.fsAgent.applyChanges(changes, {
        branch: sandboxBranch,
        dryRun: false
      });

      // Step 8: Run validation pipeline
      console.log(`[${taskId}] Running validation pipeline...`);
      const validationResult = await this.runValidationPipeline(sandboxBranch);

      // Step 9: Update confidence based on validation
      const finalConfidence = this.scorer.adjustConfidence(
        confidence,
        validationResult
      );

      // Step 10: Make deployment decision
      const decision = this.makeDeploymentDecision(finalConfidence, validationResult);

      // Step 11: Execute decision
      const outcome = await this.executeDecision(decision, {
        taskId,
        branch: sandboxBranch,
        changes,
        confidence: finalConfidence,
        validation: validationResult
      });

      // Step 12: Audit final outcome
      await this.audit.logTaskComplete({
        ...context,
        outcome,
        confidence: finalConfidence,
        filesChanged: Object.keys(changes).length
      });

      return outcome;

    } catch (error) {
      await this.audit.logTaskError(context, error);
      await this.handleFailure(taskId, error);
      throw error;
    }
  }

  async fetchRepoContext(scope = 'targeted') {
    // Intelligent context fetching based on scope
    const context = {
      structure: await this.fsAgent.getFileTree(),
      relevantFiles: []
    };

    if (scope === 'full') {
      // Full context mode - use with caution (token heavy)
      context.relevantFiles = await this.fsAgent.readFiles({
        pattern: '**/*.{js,ts,py,yml,yaml,json}',
        exclude: ['node_modules/**', 'venv/**', '.git/**'],
        maxFiles: 50
      });
    } else {
      // Targeted mode - only files likely relevant
      context.relevantFiles = await this.identifyRelevantFiles(scope);
    }

    return context;
  }

  validateKimiResponse(response) {
    // Strict validation of Kimi's output
    if (!response || typeof response !== 'object') {
      throw new Error('Invalid Kimi response: not an object');
    }

    if (!response.files || typeof response.files !== 'object') {
      throw new Error('Invalid Kimi response: missing files object');
    }

    // Validate each file change
    const validated = {};
    for (const [filepath, content] of Object.entries(response.files)) {
      // Security: prevent path traversal
      if (filepath.includes('..') || path.isAbsolute(filepath)) {
        throw new Error(`Security violation: invalid path ${filepath}`);
      }

      // Validate content
      if (typeof content !== 'string') {
        throw new Error(`Invalid content type for ${filepath}`);
      }

      validated[filepath] = content;
    }

    return validated;
  }

  checkProtectedPaths(changes) {
    const protected = [];
    for (const filepath of Object.keys(changes)) {
      if (this.protectedPaths.some(pattern => pattern.test(filepath))) {
        protected.push(filepath);
      }
    }
    return protected;
  }

  async runValidationPipeline(branch) {
    const results = {
      lint: null,
      tests: null,
      semgrep: null,
      security: null,
      overall: false
    };

    try {
      // Run all checks in parallel for speed
      const [lint, tests, semgrep] = await Promise.all([
        this.fsAgent.runCommand('npm run lint', { cwd: branch }),
        this.fsAgent.runCommand('npm test', { cwd: branch }),
        this.fsAgent.runCommand('./scripts/semgrep-check.sh', { cwd: branch })
      ]);

      results.lint = lint;
      results.tests = tests;
      results.semgrep = semgrep;
      results.overall = lint.success && tests.success && semgrep.success;

    } catch (error) {
      results.error = error.message;
      results.overall = false;
    }

    return results;
  }

  makeDeploymentDecision(confidence, validation) {
    // Decision matrix for autonomous deployment
    if (!validation.overall) {
      return { action: 'reject', reason: 'validation_failed' };
    }

    if (confidence >= 95) {
      return { action: 'auto_merge', reason: 'high_confidence' };
    }

    if (confidence >= 85) {
      return { action: 'create_pr', reason: 'good_confidence' };
    }

    if (confidence >= 70) {
      return { action: 'draft_pr', reason: 'moderate_confidence' };
    }

    return { action: 'notify_only', reason: 'low_confidence' };
  }

  async executeDecision(decision, data) {
    switch (decision.action) {
      case 'auto_merge':
        return await this.autoMerge(data);

      case 'create_pr':
        return await this.createPullRequest(data, { draft: false });

      case 'draft_pr':
        return await this.createPullRequest(data, { draft: true });

      case 'notify_only':
        return await this.notifyReview(data);

      case 'reject':
        return await this.rejectChanges(data);

      default:
        throw new Error(`Unknown decision action: ${decision.action}`);
    }
  }

  generateTaskId() {
    return `task_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  }

  buildConstraints(request) {
    return {
      maxFiles: 20,
      maxLinesPerFile: 500,
      requireTests: true,
      requireDocumentation: request.scope === 'feature',
      styleGuide: 'airbnb',
      targetComplexity: 'low'
    };
  }
}

module.exports = { TaskRouter };
```

### 2. FileSystem Agent (Production)

```javascript
// agents/fs_agent.js - Secure file system operations
const fs = require('fs').promises;
const path = require('path');
const { execSync } = require('child_process');
const prettier = require('prettier');

class FileSystemAgent {
  constructor(config) {
    this.repoRoot = config.repoRoot;
    this.tempDir = config.tempDir || './.jules/temp';
    this.maxFileSize = config.maxFileSize || 1024 * 1024; // 1MB default
  }

  async applyChanges(changes, options = {}) {
    const results = {
      success: [],
      failed: [],
      skipped: []
    };

    for (const [filepath, content] of Object.entries(changes)) {
      try {
        // Security: validate file path
        const fullPath = this.resolveSafePath(filepath);

        // Size check
        if (Buffer.byteLength(content, 'utf8') > this.maxFileSize) {
          results.skipped.push({
            file: filepath,
            reason: 'exceeds_max_size'
          });
          continue;
        }

        // Create directory if needed
        await fs.mkdir(path.dirname(fullPath), { recursive: true });

        // Format content based on file type
        const formatted = await this.formatContent(content, filepath);

        // Write file
        await fs.writeFile(fullPath, formatted, 'utf8');

        // Git add
        if (!options.dryRun) {
          execSync(`git add "${filepath}"`, { cwd: this.repoRoot });
        }

        results.success.push(filepath);

      } catch (error) {
        results.failed.push({
          file: filepath,
          error: error.message
        });
      }
    }

    return results;
  }

  async formatContent(content, filepath) {
    const ext = path.extname(filepath);

    try {
      if (['.js', '.ts', '.jsx', '.tsx'].includes(ext)) {
        return await prettier.format(content, {
          parser: 'typescript',
          semi: true,
          singleQuote: true,
          trailingComma: 'es5'
        });
      }

      if (['.json'].includes(ext)) {
        return await prettier.format(content, { parser: 'json' });
      }

      if (['.py'].includes(ext)) {
        // Use black formatter if available
        try {
          const tempFile = path.join(this.tempDir, 'temp.py');
          await fs.writeFile(tempFile, content);
          execSync(`black "${tempFile}"`, { stdio: 'ignore' });
          return await fs.readFile(tempFile, 'utf8');
        } catch {
          return content; // Fall back to original if black not available
        }
      }

    } catch (error) {
      console.warn(`Formatting failed for ${filepath}: ${error.message}`);
    }

    return content;
  }

  resolveSafePath(filepath) {
    // Prevent path traversal attacks
    const normalized = path.normalize(filepath);
    const fullPath = path.join(this.repoRoot, normalized);

    if (!fullPath.startsWith(this.repoRoot)) {
      throw new Error(`Security violation: path outside repo root: ${filepath}`);
    }

    return fullPath;
  }

  async createBranch(branchName) {
    execSync(`git checkout -b "${branchName}"`, { cwd: this.repoRoot });
    return branchName;
  }

  async getFileTree() {
    const tree = execSync('git ls-files', {
      cwd: this.repoRoot,
      encoding: 'utf8'
    });
    return tree.split('\n').filter(Boolean);
  }

  async readFiles(options) {
    // Implementation for reading multiple files efficiently
    const files = [];
    // ... (implementation details)
    return files;
  }

  async runCommand(command, options = {}) {
    try {
      const output = execSync(command, {
        cwd: options.cwd || this.repoRoot,
        encoding: 'utf8',
        stdio: 'pipe',
        timeout: options.timeout || 300000 // 5 min default
      });

      return {
        success: true,
        output,
        command
      };
    } catch (error) {
      return {
        success: false,
        error: error.message,
        output: error.stdout || error.stderr,
        command
      };
    }
  }
}

module.exports = { FileSystemAgent };
```

### 3. Confidence Scoring System

```javascript
// scoring/confidence.js - Multi-dimensional confidence calculation
class ConfidenceScorer {
  constructor(config) {
    this.baseScore = config.baseScore || 100;
    this.penalties = config.penalties || {
      todo: 10,
      fixme: 10,
      consoleLog: 5,
      debugger: 15,
      largeChange: 20,
      complexFunction: 8,
      missingTests: 25,
      lowCoverage: 15
    };
  }

  async calculateConfidence(data) {
    let score = this.baseScore;
    const breakdown = {
      base: this.baseScore,
      penalties: [],
      bonuses: []
    };

    // Analyze code quality
    score -= this.analyzeCodeQuality(data.changes, breakdown);

    // Analyze test coverage
    score -= this.analyzeTestCoverage(data.changes, breakdown);

    // Analyze change scope
    score -= this.analyzeChangeScope(data.changes, breakdown);

    // Analyze Kimi metadata
    if (data.kimiMetadata) {
      score += this.analyzeKimiConfidence(data.kimiMetadata, breakdown);
    }

    return {
      score: Math.max(0, Math.min(100, score)),
      breakdown,
      grade: this.scoreToGrade(score)
    };
  }

  analyzeCodeQuality(changes, breakdown) {
    let penalty = 0;

    for (const [filepath, content] of Object.entries(changes)) {
      // Check for TODOs/FIXMEs
      const todos = (content.match(/TODO|FIXME/gi) || []).length;
      if (todos > 0) {
        penalty += todos * this.penalties.todo;
        breakdown.penalties.push({
          type: 'todo_fixme',
          count: todos,
          penalty: todos * this.penalties.todo,
          file: filepath
        });
      }

      // Check for console.log
      const logs = (content.match(/console\.log/g) || []).length;
      if (logs > 0) {
        penalty += logs * this.penalties.consoleLog;
        breakdown.penalties.push({
          type: 'console_log',
          count: logs,
          penalty: logs * this.penalties.consoleLog,
          file: filepath
        });
      }

      // Check for debugger statements
      const debuggers = (content.match(/\bdebugger\b/g) || []).length;
      if (debuggers > 0) {
        penalty += debuggers * this.penalties.debugger;
        breakdown.penalties.push({
          type: 'debugger',
          count: debuggers,
          penalty: debuggers * this.penalties.debugger,
          file: filepath
        });
      }

      // Check function complexity (simple heuristic)
      const complexFunctions = this.detectComplexFunctions(content);
      if (complexFunctions > 0) {
        penalty += complexFunctions * this.penalties.complexFunction;
        breakdown.penalties.push({
          type: 'complex_function',
          count: complexFunctions,
          penalty: complexFunctions * this.penalties.complexFunction,
          file: filepath
        });
      }
    }

    return penalty;
  }

  analyzeTestCoverage(changes, breakdown) {
    let penalty = 0;

    // Check if test files are included
    const hasTestFiles = Object.keys(changes).some(f =>
      f.includes('.test.') || f.includes('.spec.') || f.includes('__tests__')
    );

    if (!hasTestFiles) {
      penalty += this.penalties.missingTests;
      breakdown.penalties.push({
        type: 'missing_tests',
        penalty: this.penalties.missingTests
      });
    }

    return penalty;
  }

  analyzeChangeScope(changes, breakdown) {
    let penalty = 0;
    const fileCount = Object.keys(changes).length;

    // Large change sets are riskier
    if (fileCount > 15) {
      penalty += this.penalties.largeChange;
      breakdown.penalties.push({
        type: 'large_change',
        fileCount,
        penalty: this.penalties.largeChange
      });
    }

    return penalty;
  }

  analyzeKimiConfidence(metadata, breakdown) {
    // Bonus points if Kimi expressed high confidence
    if (metadata.confidence && metadata.confidence === 'high') {
      breakdown.bonuses.push({
        type: 'kimi_confidence',
        bonus: 5
      });
      return 5;
    }
    return 0;
  }

  adjustConfidence(initialScore, validationResult) {
    let adjusted = initialScore.score;

    // Adjust based on validation results
    if (validationResult.overall) {
      adjusted += 10; // Bonus for passing all checks
    } else {
      if (!validationResult.lint.success) adjusted -= 15;
      if (!validationResult.tests.success) adjusted -= 20;
      if (!validationResult.semgrep.success) adjusted -= 25;
    }

    return {
      ...initialScore,
      score: Math.max(0, Math.min(100, adjusted)),
      adjusted: true,
      validation: validationResult
    };
  }

  detectComplexFunctions(content) {
    // Simple heuristic: functions with > 50 lines or > 5 nested blocks
    const functions = content.match(/function\s+\w+\s*\([^)]*\)\s*{/g) || [];
    return functions.length > 5 ? Math.floor(functions.length / 5) : 0;
  }

  scoreToGrade(score) {
    if (score >= 95) return 'A+';
    if (score >= 90) return 'A';
    if (score >= 85) return 'B+';
    if (score >= 80) return 'B';
    if (score >= 70) return 'C';
    return 'D';
  }
}

module.exports = { ConfidenceScorer };
```

### 4. Deploy Pipeline (Bash)

```bash
#!/usr/bin/env bash
# deploy.sh - Comprehensive validation pipeline

set -e  # Exit on any error
set -o pipefail  # Catch errors in pipes

# Color output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

echo_info() { echo -e "${GREEN}[INFO]${NC} $1"; }
echo_warn() { echo -e "${YELLOW}[WARN]${NC} $1"; }
echo_error() { echo -e "${RED}[ERROR]${NC} $1"; }

REPO_ROOT=$(git rev-parse --show-toplevel)
cd "$REPO_ROOT"

# Create results directory
RESULTS_DIR="./.jules/validation-results/$(date +%Y%m%d-%H%M%S)"
mkdir -p "$RESULTS_DIR"

echo_info "Starting validation pipeline..."
echo_info "Results will be saved to: $RESULTS_DIR"

# ============================================================
# STAGE 1: Code Formatting
# ============================================================
echo_info "Stage 1: Code Formatting"

if command -v prettier &> /dev/null; then
  echo_info "Running Prettier..."
  prettier --check "**/*.{js,ts,jsx,tsx,json,yml,yaml}" \
    > "$RESULTS_DIR/prettier.log" 2>&1 || {
    echo_warn "Prettier found formatting issues (auto-fixable)"
  }
else
  echo_warn "Prettier not found, skipping"
fi

# ============================================================
# STAGE 2: Linting
# ============================================================
echo_info "Stage 2: Linting"

if [ -f "package.json" ]; then
  echo_info "Running ESLint..."
  npm run lint -- --format json > "$RESULTS_DIR/eslint.json" 2>&1 || {
    echo_error "ESLint found issues"
    cat "$RESULTS_DIR/eslint.json"
    exit 1
  }
fi

if [ -f "requirements.txt" ] || [ -f "pyproject.toml" ]; then
  if command -v pylint &> /dev/null; then
    echo_info "Running Pylint..."
    pylint **/*.py > "$RESULTS_DIR/pylint.txt" 2>&1 || {
      echo_warn "Pylint found issues (score below threshold)"
    }
  fi
fi

# ============================================================
# STAGE 3: Security Scanning (Semgrep)
# ============================================================
echo_info "Stage 3: Security Scanning"

if command -v semgrep &> /dev/null; then
  echo_info "Running Semgrep..."

  # Run with custom rules
  semgrep --config=./.semgrep/rules.yml \
    --json \
    --output="$RESULTS_DIR/semgrep.json" \
    --severity ERROR \
    --severity WARNING \
    . || {

    echo_error "Semgrep found security issues:"

    # Parse and display critical findings
    python3 << 'EOF'
import json
import sys

try:
    with open('./.jules/validation-results/latest/semgrep.json') as f:
        data = json.load(f)
        errors = [r for r in data.get('results', []) if r['extra']['severity'] == 'ERROR']

        if errors:
            print(f"\n{len(errors)} CRITICAL issues found:\n")
            for err in errors[:5]:  # Show first 5
                print(f"  â€¢ {err['check_id']}")
                print(f"    {err['path']}:{err['start']['line']}")
                print(f"    {err['extra']['message']}\n")
            sys.exit(1)
except Exception as e:
    print(f"Error parsing semgrep results: {e}", file=sys.stderr)
    sys.exit(1)
EOF
  }

  echo_info "Semgrep: No critical issues found"
else
  echo_error "Semgrep not installed. Install with: pip install semgrep"
  exit 1
fi

# ============================================================
# STAGE 4: Unit Tests
# ============================================================
echo_info "Stage 4: Unit Tests"

if [ -f "package.json" ]; then
  echo_info "Running Jest tests..."
  npm test -- --coverage --json --outputFile="$RESULTS_DIR/jest.json" || {
    echo_error "Tests failed"
    exit 1
  }

  # Check coverage threshold
  COVERAGE=$(jq '.coverageMap | length' "$RESULTS_DIR/jest.json" 2>/dev/null || echo "0")
  echo_info "Test coverage: $COVERAGE%"
fi

if [ -f "pytest.ini" ] || [ -f "pyproject.toml" ]; then
  echo_info "Running pytest..."
  pytest --cov --cov-report=json:"$RESULTS_DIR/coverage.json" || {
    echo_error "Tests failed"
    exit 1
  }
fi

# ============================================================
# STAGE 5: Dependency Audit
# ============================================================
echo_info "Stage 5: Dependency Audit"

if [ -f "package-lock.json" ]; then
  echo_info "Auditing npm dependencies..."
  npm audit --json > "$RESULTS_DIR/npm-audit.json" || {
    CRITICAL=$(jq '.metadata.vulnerabilities.critical' "$RESULTS_DIR/npm-audit.json")
    HIGH=$(jq '.metadata.vulnerabilities.high' "$RESULTS_DIR/npm-audit.json")

    if [ "$CRITICAL" -gt 0 ] || [ "$HIGH" -gt 5 ]; then
      echo_error "Critical vulnerabilities found: $CRITICAL critical, $HIGH high"
      exit 1
    fi

    echo_warn "Some vulnerabilities found but below threshold"
  }
fi

# ============================================================
# STAGE 6: Integration Tests (if applicable)
# ============================================================
if [ -f "docker-compose.test.yml" ]; then
  echo_info "Stage 6: Integration Tests"
  docker-compose -f docker-compose.test.yml up --abort-on-container-exit || {
    echo_error "Integration tests failed"
    exit 1
  }
  docker-compose -f docker-compose.test.yml down
fi

# ============================================================
# STAGE 7: Build Verification
# ============================================================
echo_info "Stage 7: Build Verification"

if [ -f "package.json" ]; then
  echo_info "Building application..."
  npm run build > "$RESULTS_DIR/build.log" 2>&1 || {
    echo_error "Build failed"
    cat "$RESULTS_DIR/build.log"
    exit 1
  }
fi

# ============================================================
# SUCCESS
# ============================================================
echo ""
echo_info "========================================="
echo_info "  ALL VALIDATION CHECKS PASSED âœ“"
echo_info "========================================="
echo_info "Results saved to: $RESULTS_DIR"
echo ""

# Create symlink to latest results
ln -sf "$RESULTS_DIR" "./.jules/validation-results/latest"

# Generate summary report
cat > "$RESULTS_DIR/SUMMARY.md" << EOF
# Validation Summary

**Date**: $(date)
**Branch**: $(git branch --show-current)
**Commit**: $(git rev-parse HEAD)

## Results

âœ… Code Formatting: Passed
âœ… Linting: Passed
âœ… Security Scan: Passed
âœ… Unit Tests: Passed
âœ… Dependency Audit: Passed
âœ… Build: Passed

## Details

See individual log files in this directory for detailed results.
EOF

echo_info "Summary report generated: $RESULTS_DIR/SUMMARY.md"

exit 0
```

### 5. Semgrep Rules Configuration

```yaml
# .semgrep/rules.yml - Custom security and quality rules
rules:
  # ============================================================
  # Security Rules
  # ============================================================
  - id: no-hardcoded-secrets
    patterns:
      - pattern-either:
          - pattern: |
              $VAR = "...password..."
          - pattern: |
              $VAR = "...api_key..."
          - pattern: |
              $VAR = "...secret..."
          - pattern: |
              $VAR = "...token..."
    message: "Potential hardcoded secret detected"
    severity: ERROR
    languages: [javascript, typescript, python]

  - id: sql-injection-risk
    patterns:
      - pattern: |
          $QUERY = $INPUT + "..."
      - pattern: |
          cursor.execute($INPUT + "...")
    message: "Potential SQL injection vulnerability"
    severity: ERROR
    languages: [python, javascript]

  - id: unsafe-eval
    patterns:
      - pattern: eval($INPUT)
      - pattern: Function($INPUT)
    message: "Unsafe use of eval() or Function() constructor"
    severity: ERROR
    languages: [javascript, typescript]

  # ============================================================
  # Code Quality Rules
  # ============================================================
  - id: no-console-log
    patterns:
      - pattern: console.log(...)
      - pattern: console.debug(...)
    message: "console.log found - use proper logging framework"
    severity: WARNING
    languages: [javascript, typescript]

  - id: no-todo-fixme
    patterns:
      - pattern-regex: "TODO|FIXME"
    message: "TODO/FIXME found in code"
    severity: WARNING
    languages: [javascript, typescript, python]

  - id: no-debugger
    pattern: debugger
    message: "debugger statement found"
    severity: ERROR
    languages: [javascript, typescript]

  - id: unused-imports
    patterns:
      - pattern: |
          import $X from "..."
          ...
      - pattern-not: $X
    message: "Unused import detected"
    severity: WARNING
    languages: [javascript, typescript]

  # ============================================================
  # Best Practices
  # ============================================================
  - id: missing-error-handling
    patterns:
      - pattern: |
          await $FUNC(...)
      - pattern-not-inside: |
          try {
            ...
          } catch (...) {
            ...
          }
    message: "Async operation without error handling"
    severity: WARNING
    languages: [javascript, typescript]

  - id: broad-exception-catching
    pattern: |
      except:
        pass
    message: "Broad exception catching without handling"
    severity: WARNING
    languages: [python]
```

---

## Self-Healing Task Implementation

### Alert Handler

```javascript
// handlers/self_healing.js - Autonomous incident response
const { TaskRouter } = require('../route_task');

class SelfHealingHandler {
  constructor(config) {
    this.router = new TaskRouter(config.router);
    this.prometheusUrl = config.prometheusUrl;
    this.alertThresholds = config.alertThresholds || {
      criticalAutoDeploy: false,
      highAutoDeploy: false,
      mediumAutoFix: true
    };
  }

  async handlePrometheusAlert(alert) {
    const context = {
      alertId: alert.labels.alertname,
      severity: alert.labels.severity,
      service: alert.labels.service,
      summary: alert.annotations.summary,
      description: alert.annotations.description,
      timestamp: alert.startsAt
    };

    console.log(`ðŸš¨ Alert received: ${context.alertId} (${context.severity})`);

    // Step 1: Analyze alert to determine if auto-fix is appropriate
    const analysis = await this.analyzeAlert(context);

    if (!analysis.autoFixable) {
      console.log(`âš ï¸  Alert ${context.alertId} requires human intervention`);
      await this.notifyOncall(context, analysis);
      return { action: 'escalated', analysis };
    }

    // Step 2: Create diagnostic task
    console.log(`ðŸ” Creating diagnostic task for ${context.service}`);

    const diagnosticResult = await this.router.routeTask({
      intent: `Diagnose and propose fix for: ${context.summary}.
               Service: ${context.service}
               Error: ${context.description}

               Requirements:
               1. Analyze recent logs and metrics
               2. Identify root cause
               3. Propose minimal fix
               4. Generate regression test
               5. Document the fix`,

      scope: 'targeted',
      requestor: 'self-healing-system',
      metadata: {
        alertContext: context,
        autoDeployAllowed: this.shouldAutoDeploy(context.severity)
      }
    });

    // Step 3: Handle result
    if (diagnosticResult.success && diagnosticResult.confidence.score >= 85) {
      console.log(`âœ… Self-healing completed for ${context.alertId}`);
      return {
        action: 'resolved',
        taskId: diagnosticResult.taskId,
        confidence: diagnosticResult.confidence,
        prUrl: diagnosticResult.prUrl
      };
    } else {
      console.log(`âš ï¸  Self-healing attempted but requires review`);
      await this.notifyOncall(context, {
        ...analysis,
        attemptedFix: diagnosticResult
      });
      return {
        action: 'needs_review',
        diagnosticResult
      };
    }
  }

  async analyzeAlert(context) {
    // Determine if alert is suitable for automated fixing
    const autoFixablePatterns = [
      /service.*not responding/i,
      /high memory usage/i,
      /connection pool exhausted/i,
      /rate limit exceeded/i,
      /cache miss ratio high/i
    ];

    const requiresHumanPatterns = [
      /data corruption/i,
      /security breach/i,
      /payment.*failed/i,
      /database.*down/i
    ];

    const description = context.description.toLowerCase();

    if (requiresHumanPatterns.some(p => p.test(description))) {
      return {
        autoFixable: false,
        reason: 'critical_system_affected',
        recommendation: 'immediate_human_intervention'
      };
    }

    if (autoFixablePatterns.some(p => p.test(description))) {
      return {
        autoFixable: true,
        confidence: 'medium',
        estimatedFixTime: '5-10 minutes'
      };
    }

    return {
      autoFixable: false,
      reason: 'unknown_pattern',
      recommendation: 'manual_investigation'
    };
  }

  shouldAutoDeploy(severity) {
    switch (severity) {
      case 'critical':
        return this.alertThresholds.criticalAutoDeploy;
      case 'high':
        return this.alertThresholds.highAutoDeploy;
      case 'medium':
        return this.alertThresholds.mediumAutoFix;
      default:
        return false;
    }
  }

  async notifyOncall(context, analysis) {
    // Implementation for PagerDuty/Slack/etc notifications
    console.log(`ðŸ“¢ Notifying oncall: ${context.alertId}`);
    // ... notification logic ...
  }
}

module.exports = { SelfHealingHandler };
```

---

## Operational Guidelines

### 1. Monitoring & Observability

**Key Metrics to Track:**
- Task success rate (target: >90%)
- Average confidence score (target: >85)
- Validation pipeline pass rate (target: >95%)
- Time to resolution (target: <15 minutes)
- Human intervention rate (target: <20%)

**Audit Log Structure:**
```json
{
  "taskId": "task_1736064000_abc123",
  "timestamp": "2026-01-05T10:30:00Z",
  "event": "task_completed",
  "metadata": {
    "intent": "Fix memory leak in user service",
    "filesChanged": 3,
    "confidence": 92,
    "validationPassed": true,
    "deploymentAction": "auto_merge",
    "duration_ms": 127000
  }
}
```

### 2. Rollback Procedures

**Automatic Rollback Triggers:**
- Post-deployment error rate increase >50%
- Service health check failures
- Critical alerts within 5 minutes of deploy

**Manual Rollback:**
```bash
# Revert specific task deployment
./scripts/rollback-task.sh <task_id>

# Full system rollback
./scripts/emergency-rollback.sh
```

### 3. Security Considerations

**Protected Paths (Always Require Human Review):**
- `.github/workflows/**` - CI/CD pipelines
- `config/**` - Configuration files
- `**/auth/**` - Authentication logic
- `**/payment/**` - Payment processing
- `docker-compose*.yml` - Infrastructure
- `package.json`, `requirements.txt` - Dependencies

**Audit Requirements:**
- All AI-generated changes logged with full diff
- Deployment decisions recorded with reasoning
- Confidence scores stored for trend analysis
- Security scan results retained for 90 days

### 4. Governance Model

**Approval Matrix:**

| Confidence Score | Files Changed | Action | Human Review |
|-----------------|---------------|---------|--------------|
| 95-100 | 1-10 | Auto-merge | Optional |
| 85-94 | 1-10 | Create PR | Recommended |
| 85-100 | 11-20 | Create PR | Required |
| 70-84 | Any | Draft PR | Required |
| <70 | Any | Notify only | Required |
| Any | Protected paths | Always block | Always required |

---

## Next Steps: Implementation Checklist

### Phase 1: Foundation (Week 1)
- [ ] Set up repository structure (`.jules/` directory)
- [ ] Install and configure Semgrep
- [ ] Implement FileSystemAgent with security controls
- [ ] Create basic deploy.sh pipeline
- [ ] Set up audit logging infrastructure

### Phase 2: Core Integration (Week 2)
- [ ] Implement TaskRouter with full orchestration
- [ ] Integrate Kimi API client
- [ ] Build ConfidenceScorer with all dimensions
- [ ] Create protected paths validation
- [ ] Implement PR creation automation

### Phase 3: Self-Healing (Week 3)
- [ ] Set up Prometheus alert webhook
- [ ] Implement SelfHealingHandler
- [ ] Create alert analysis logic
- [ ] Build oncall notification system
- [ ] Test with simulated incidents

### Phase 4: Production Hardening (Week 4)
- [ ] Add comprehensive error handling
- [ ] Implement rollback automation
- [ ] Create monitoring dashboards
- [ ] Document runbooks
- [ ] Conduct security review
- [ ] Load testing and validation

---

## Configuration Template

```javascript
// config/production.js
module.exports = {
  kimi: {
    apiKey: process.env.KIMI_API_KEY,
    endpoint: 'https://api.kimi.moonshot.cn/v1',
    maxContextTokens: 128000,
    timeout: 60000
  },

  fsAgent: {
    repoRoot: process.env.REPO_ROOT || '/app/repo',
    tempDir: './.jules/temp',
    maxFileSize: 1024 * 1024 // 1MB
  },

  scoring: {
    baseScore: 100,
    penalties: {
      todo: 10,
      fixme: 10,
      consoleLog: 5,
      debugger: 15,
      largeChange: 20,
      complexFunction: 8,
      missingTests: 25,
      lowCoverage: 15
    }
  },

  audit: {
    logDir: './.jules/logs',
    retentionDays: 90,
    enableDetailedDiffs: true
  },

  protectedPaths: [
    /^\.github\/workflows/,
    /^config\//,
    /^.*auth.*\./,
    /^.*payment.*\./,
    /docker-compose\.yml$/,
    /package\.json$/,
    /requirements\.txt$/
  ],

  deployment: {
    autoMergeThreshold: 95,
    prCreationThreshold: 85,
    draftPrThreshold: 70,
    maxFilesForAutoMerge: 10
  },

  selfHealing: {
    enabled: true,
    alertThresholds: {
      criticalAutoDeploy: false,
      highAutoDeploy: false,
      mediumAutoFix: true
    }
  }
};
```

---

## Success Criteria

**System is production-ready when:**
1. âœ… All validation stages pass consistently
2. âœ… Confidence scoring accurately predicts deployment safety
3. âœ… Protected paths cannot be modified without human approval
4. âœ… Rollback procedures tested and documented
5. âœ… Monitoring dashboards operational
6. âœ… Security audit completed
7. âœ… Oncall team trained on intervention procedures
8. âœ… Self-healing successfully resolves test incidents

**Performance Targets:**
- Task processing time: <3 minutes (p95)
- Validation pipeline: <5 minutes (p95)
- False positive rate: <5%
- Rollback time: <2 minutes

---

## Support & Troubleshooting

**Common Issues:**

1. **High false positive rate**
   - Solution: Adjust confidence scoring penalties
   - Review Semgrep rules for overly strict patterns

2. **Tasks timing out**
   - Solution: Reduce context window size
   - Implement incremental file fetching

3. **Protected path conflicts**
   - Solution: Review protected path patterns
   - Create exception process for legitimate changes

4. **Low confidence scores**
   - Solution: Improve Kimi prompts with more context
   - Add more sophisticated code quality analysis

**Emergency Contacts:**
- System Owner: [Your Team]
- Security Team: [Security Contact]
- Kimi API Support: [Vendor Support]

---

*This document should be treated as a living specification. Update as the system evolves and new patterns emerge.*