import unittest
import os
import sys
from unittest.mock import patch, MagicMock

# --- Path Setup ---
# Add repo root to resolve 'agents' import
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
# Add 'vaal-ai-empire' to resolve its internal absolute imports (e.g., 'from api...')
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../vaal-ai-empire')))


# --- Dynamic Imports ---
# Now that paths are set, we can import everything
from autogen import LLMConfig
from agents.ag2.orchestrator import VaalAIEmpireOrchestrator
from services.content_generator import ContentFactory, _get_cached_providers


class TestOrchestratorConfig(unittest.TestCase):
    # This class remains to test the previously submitted orchestrator changes
    @patch.dict(os.environ, {"OPENAI_MODEL": "test-model-from-env", "OPENAI_API_KEY": "test-key-from-env"})
    @patch("agents.ag2.orchestrator.LLMConfig.from_json")
    def test_load_from_env_vars_when_file_fails(self, mock_from_json):
        mock_from_json.side_effect = ValueError("File not found")
        orchestrator = VaalAIEmpireOrchestrator()
        config_list = orchestrator.llm_config.config_list
        self.assertEqual(len(config_list), 1)
        self.assertEqual(config_list[0]["model"], "test-model-from-env")

    @patch("agents.ag2.orchestrator.LLMConfig.from_json")
    def test_load_from_file_successfully(self, mock_from_json):
        mock_from_json.return_value = LLMConfig(config_list=[{"model": "file-model", "api_key": "file-key"}])
        orchestrator = VaalAIEmpireOrchestrator()
        config_list = orchestrator.llm_config.config_list
        self.assertEqual(len(config_list), 1)
        self.assertEqual(config_list[0]["model"], "file-model")

    @patch.dict(os.environ, {}, clear=True)
    @patch("agents.ag2.orchestrator.LLMConfig.from_json")
    def test_fallback_to_dummy_config(self, mock_from_json):
        mock_from_json.side_effect = ValueError("File not found")
        orchestrator = VaalAIEmpireOrchestrator()
        config_list = orchestrator.llm_config.config_list
        self.assertEqual(len(config_list), 1)
        self.assertEqual(config_list[0]["model"], "gpt-4")


class TestContentFactoryKimiVLLM(unittest.TestCase):

    @patch.dict(os.environ, {"KIMI_VLLM_ENDPOINT": "http://mock-vllm-endpoint:8000/v1"})
    @patch("api.kimi.OpenAI") # Patch OpenAI where it's used in kimi.py
    def test_kimi_provider_uses_vllm_client(self, mock_openai_client):
        """
        Tests that ContentFactory correctly uses the vLLM-based KimiAPI.
        """
        # Arrange: Mock the OpenAI client that KimiAPI will instantiate
        mock_client = mock_openai_client.return_value
        mock_response = MagicMock()
        mock_response.choices = [MagicMock(message=MagicMock(content="Generated by Kimi on vLLM"))]
        mock_response.usage = MagicMock(prompt_tokens=10, completion_tokens=5, total_tokens=15)
        mock_client.chat.completions.create.return_value = mock_response

        # Clear the cache to force re-initialization of providers
        _get_cached_providers.cache_clear()

        # Act: Instantiate ContentFactory which will create our mocked KimiAPI
        factory = ContentFactory()

        # Make other providers unavailable to ensure Kimi is chosen
        factory.providers["groq"] = None
        factory.providers["cohere"] = None
        factory.providers["mistral"] = None

        self.assertIsNotNone(factory.providers.get("kimi"), "KimiAPI failed to initialize.")

        result = factory.generate_content("test prompt")

        # Assert
        mock_openai_client.assert_called_once_with(
            base_url="http://mock-vllm-endpoint:8000/v1", api_key="not-needed"
        )
        mock_client.chat.completions.create.assert_called_once()
        self.assertEqual(result["provider"], "kimi")
        self.assertEqual(result["text"], "Generated by Kimi on vLLM")
        self.assertEqual(result["tokens"], 5)


if __name__ == "__main__":
    unittest.main()
