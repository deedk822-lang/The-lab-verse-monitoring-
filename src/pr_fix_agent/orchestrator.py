"""
Production Orchestrator - Fixes timeout and chunking issues
"""
import argparse
import json
import logging
import sys
from pathlib import Path
from typing import List, Dict, Optional
import re

# Configure logging first
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# Try to import ollama - fallback to mock if not available
try:
    import ollama
    HAS_OLLAMA = True
except ImportError:
    HAS_OLLAMA = False
    logger.warning("Ollama not installed - using mock mode")


class ChunkedOllamaClient:
    """
    Ollama client with chunking to prevent timeouts
    Fixes: 14KB prompt timeout, 120s hang issues
    """

    def __init__(self, reasoning_model: str = "deepseek-r1:1.5b",
                 coding_model: str = "qwen2.5-coder:1.5b"):
        self.reasoning_model = reasoning_model
        self.coding_model = coding_model
        self.max_prompt_size = 4000  # âœ… FIX: Prevent 14KB prompts
        self.timeout = 90

        if not HAS_OLLAMA:
            logger.warning("Running in mock mode - no actual LLM calls")

    def query(self, model: str, prompt: str) -> Dict:
        """Query with timeout protection and truncation"""
        # âœ… FIX: Truncate large prompts
        if len(prompt) > self.max_prompt_size:
            logger.warning(f"Truncating prompt from {len(prompt)} to {self.max_prompt_size}")
            prompt = prompt[:self.max_prompt_size] + "\n... [truncated]"

        if not HAS_OLLAMA:
            # Mock mode for testing
            if "Provide JSON" in prompt:
                 return {
                    'success': True,
                    'content': '```json\n{"root_cause": "Mock analysis", "fix_approach": "Mock fix", "risk_level": "low"}\n```',
                    'model': model
                }
            return {
                'success': True,
                'content': 'Mock response',
                'model': model
            }

        try:
            logger.info(f"Querying {model} ({len(prompt)} chars)")

            response = ollama.chat(
                model=model,
                messages=[{'role': 'user', 'content': prompt}],
                options={
                    'temperature': 0.1,
                    'num_ctx': 4096,
                    'num_predict': 2000
                }
            )

            return {
                'success': True,
                'content': response['message']['content'],
                'model': model
            }

        except Exception as e:
            logger.error(f"Query failed: {e}")
            return {
                'success': False,
                'error': str(e),
                'model': model
            }


class FixOrchestrator:
    """Main orchestrator"""

    def __init__(self, reasoning_model: str = "deepseek-r1:1.5b", coding_model: str = "qwen2.5-coder:1.5b"):
        self.client = ChunkedOllamaClient(reasoning_model, coding_model)

    def parse_findings(self, analysis_path: Path) -> List[Dict]:
        """Parse findings with multiple format support"""
        findings = []

        try:
            if not analysis_path.exists():
                logger.error(f"Findings file not found: {analysis_path}")
                return []

            with open(analysis_path) as f:
                content = f.read()

            # Try JSON first
            try:
                data = json.loads(content)
                if isinstance(data, list):
                    return data
                elif isinstance(data, dict):
                    return data.get('findings', data.get('results', []))
            except json.JSONDecodeError:
                pass

            # Parse text format (bandit, mypy, etc.)
            pattern = r'([^:]+):([0-9]+):([0-9]*):?\s*(\w+):?\s*(.*)'
            for line in content.split('\n')[:50]:
                match = re.match(pattern, line)
                if match:
                    findings.append({
                        'file': match.group(1),
                        'line': int(match.group(2)),
                        'severity': 'medium',
                        'issue': match.group(5),
                        'suggestion': 'See documentation'
                    })

        except Exception as e:
            logger.error(f"Parse error: {e}")

        return findings

    def extract_json(self, text: str) -> Optional[Dict]:
        """Safely extract JSON from LLM response"""
        try:
            # Look for triple backticks
            if '```json' in text:
                text = text.split('```json')[1].split('```')[0]
            elif '```' in text:
                text = text.split('```')[1].split('```')[0]

            # Clean text
            text = text.strip()

            # Attempt to find the first '{' and last '}'
            start = text.find('{')
            end = text.rfind('}')
            if start != -1 and end != -1:
                json_str = text[start:end+1]
                return json.loads(json_str)
        except Exception as e:
            logger.warning(f"Failed to extract JSON: {e}")
        return None

    def analyze_finding(self, finding: Dict) -> Dict:
        """Analyze with reasoning model"""
        prompt = f"""Analyze this issue:
File: {finding.get('file', 'unknown')}
Line: {finding.get('line', 0)}
Issue: {finding.get('issue', 'unknown')}

Provide JSON:
{{"root_cause": "brief", "fix_approach": "how to fix", "risk_level": "low"}}"""

        result = self.client.query(self.client.reasoning_model, prompt)

        if result['success']:
            analysis = self.extract_json(result['content'])
            if analysis:
                return analysis

        return {
            'root_cause': 'Analysis failed',
            'fix_approach': 'Manual review required',
            'risk_level': 'high'
        }

    def implement_fix(self, proposal: Dict) -> Dict:
        """Implement fix with coding model"""
        finding = proposal.get('finding', {})
        file_path = Path(finding.get('file', ''))

        if not file_path.exists():
             return {'success': False, 'error': f"File not found: {file_path}"}

        try:
            original_code = file_path.read_text()

            prompt = f"""Fix the following issue in this code:
Issue: {finding.get('issue')}
Fix Approach: {proposal.get('fix_approach')}

Code:
```python
{original_code}
```

Provide ONLY the full fixed code for this file."""

            result = self.client.query(self.client.coding_model, prompt)

            if result['success']:
                fixed_code = result['content']
                if '```python' in fixed_code:
                    fixed_code = fixed_code.split('```python')[1].split('```')[0]
                elif '```' in fixed_code:
                    fixed_code = fixed_code.split('```')[1].split('```')[0]

                return {
                    'success': True,
                    'original': original_code,
                    'fixed': fixed_code.strip()
                }
        except Exception as e:
            logger.error(f"Fix implementation failed: {e}")

        return {'success': False, 'error': "LLM failed to generate fix"}


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('mode', choices=['review', 'fix', 'reasoning', 'coding', 'generate-pr'], default='review', nargs='?')
    parser.add_argument('--findings', '-f', default='analysis-results/safety.json')
    parser.add_argument('--proposals', help='Path to proposals JSON')
    parser.add_argument('--test-results', help='Path to test results JSON')
    parser.add_argument('--output', help='Output file')
    parser.add_argument('--limit', type=int, default=10)
    parser.add_argument('--apply', action='store_true')
    parser.add_argument('--reasoning-model', default="deepseek-r1:1.5b")
    parser.add_argument('--coding-model', default="qwen2.5-coder:1.5b")
    args = parser.parse_args()

    orch = FixOrchestrator(args.reasoning_model, args.coding_model)

    # Backward compatibility for 'reasoning' mode
    if args.mode == 'reasoning':
        args.mode = 'review'
        if args.findings and Path(args.findings).is_dir():
             # If a directory is passed, check for various finding files
             findings_dir = Path(args.findings)
             findings = []
             for f in findings_dir.glob("*.json"):
                 findings.extend(orch.parse_findings(f))

             proposals = []
             for i, finding in enumerate(findings[:args.limit]):
                 logger.info(f"Analyzing {i+1}/{len(findings[:args.limit])}")
                 analysis = orch.analyze_finding(finding)
                 proposals.append({'finding': finding, **analysis})

             output_file = args.output or 'proposals.json'
             with open(output_file, 'w') as f:
                 json.dump(proposals, f, indent=2)
             logger.info(f"Saved {len(proposals)} proposals to {output_file}")
             return

    if args.mode == 'review':
        findings = orch.parse_findings(Path(args.findings))
        logger.info(f"Found {len(findings)} findings")

        findings = findings[:args.limit]
        proposals = []
        for i, finding in enumerate(findings):
            logger.info(f"Analyzing {i+1}/{len(findings)}")
            analysis = orch.analyze_finding(finding)
            proposals.append({'finding': finding, **analysis})

        output_file = args.output or 'proposals.json'
        with open(output_file, 'w') as f:
            json.dump(proposals, f, indent=2)
        logger.info(f"Saved {len(proposals)} proposals to {output_file}")

    elif args.mode in ['coding', 'fix']:
        proposals_path = Path(args.proposals or 'proposals.json')
        if not proposals_path.exists():
            logger.error(f"Proposals file not found: {proposals_path}")
            sys.exit(1)

        with open(proposals_path) as f:
            proposals = json.load(f)

        for i, proposal in enumerate(proposals[:args.limit]):
            logger.info(f"Implementing fix {i+1}/{len(proposals[:args.limit])}")
            fix = orch.implement_fix(proposal)
            if fix['success'] and args.apply:
                file_path = proposal['finding']['file']
                if fix['fixed'] and len(fix['fixed']) > 10:
                    with open(file_path, 'w') as f:
                        f.write(fix['fixed'])
                    logger.info(f"Applied fix to {file_path}")
                else:
                    logger.warning(f"Refusing to apply empty or too short fix to {file_path}")

    elif args.mode == 'generate-pr':
        output_file = args.output or 'pr-body.md'
        proposals_path = Path(args.proposals or 'proposals.json')

        body = "# ðŸ¤– Automated Code Review Fixes\n\n"
        if proposals_path.exists():
            with open(proposals_path) as f:
                proposals = json.load(f)
            body += "## Fixes Proposed\n"
            for p in proposals:
                body += f"- **{p['finding']['file']}**: {p['finding']['issue']}\n"
                body += f"  - *Root Cause*: {p.get('root_cause', 'N/A')}\n"
        else:
            body += "Analysis completed. Check logs for details.\n"

        with open(output_file, 'w') as f:
            f.write(body)
        logger.info(f"Generated PR body in {output_file}")


if __name__ == "__main__":
    main()
